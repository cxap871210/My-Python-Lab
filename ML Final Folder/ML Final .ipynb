{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主題：OpenAI Gym 遊戲實作\n",
    "[組員]  \n",
    "邱廷翔 B06705023  \n",
    "蕭昀豪 B06705030  \n",
    "陳柄瑞 B07705052  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole  \n",
    "CartPole是OpenAI Gym中最簡單的一個遊戲，遊戲目標是藉由移動車子來保持上方木棒平衡。  \n",
    "#<img src=\"cartpole.mp4\" width=\"400\">\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/watch?v=zBSYCz8UbTk\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此遊戲的observation只有4種、action只有2種(控制車子左右)。  \n",
    "先用隨機選取action的方式進行遊戲，若分數超過某定值則將當時環境中的observation、當下做出的action記下。重複多次後將這些資料當作訓練資料訓練一個神經網路。  \n",
    "最後進行遊戲時，把環境中的observation當作輸入，用訓練出的神經網路預測action進行遊戲。成功將此遊戲破關。(此遊戲破關的定義為：連續195場，拿下195分以上)  \n",
    "程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-pedalwalker  \n",
    "Bi-pedalwalker是OpenAI Gym中另一個較Cartpole複雜的遊戲，遊戲方式為控制機器人腳步，使得機器人能走越遠越好。   \n",
    "#<img src=\"pedalwalker.mp4\" width=\"400\">  \n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/watch?v=ZtkkJJQ-x3U\" frameborder=\"0\" allowfullscreen></iframe>')  \n",
    "    此遊戲的observation共計23種、action則有4種(控制機器人腳步)。  \n",
    "    我們實做Bi-pedalwalker的過程可分為以下3階段：\n",
    "    1. 使用與進行cartpole時一樣的策略進行遊戲\n",
    "    2. 發現誤把Time當作reward\n",
    "    3. 發現Q-function不含有reward\n",
    "    \n",
    "首先我們使用與進行Cartpole時一樣的方式進行遊戲，一開始即取得良好成果。然而當使用env.render()指令將遊戲畫面視覺化時，我們發現因為我們誤把遊戲時間當成reward，導致機器人的遊戲策略為「如何能存活最久」，而非「如何走得最遠」。最終結果為機器人選擇按兵不動，藉此避免跌倒延長存活時間。\n",
    "    \n",
    "在更正上述盲點後，我們發現\n",
    "而將reward修正後執行結果依然不甚理想，在train的過程中，walker的表現沒有辦法往合理的方向收斂。  \n",
    "以下為我們原先認為的原因：  \n",
    "一開始收集資料的方式為隨機生成四個action，再紀錄得到的reward。而此遊戲的四個action皆為介於-1與1的連續值，因此在一開始產出的學習數據中無法確保有足夠好的數據讓神經網絡學習，也就是隨機生成資料時reward都相當低，很少有資料reward會高到被拿去訓練。  \n",
    "但後來我們發現有個更大的問題：  \n",
    "原先的神經網絡是以state為輸入來預測action，但如此一來神經網絡僅能學會「隨機生成資料時，各種state與action的隨機組合」，也就是在這個神經網絡沒有reward參與的情況下，沒有評斷action的標準。因此神經網絡學不會「怎樣才是好的應對方式」。而這個問題將在之後被修正。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LunarLander-v2  \n",
    "LunarLander-v2亦為OpenAI Gym中的遊戲，遊戲方式為控制太空船不同方向的引擎，使太空船順利降落在兩旗竿之間。    \n",
    "#<img src=\"lunarlander.mp4\" width=\"400\">  \n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/watch?v=R3w7CQDmf5A\" frameborder=\"0\" allowfullscreen></iframe>')  \n",
    "此遊戲的observation有8種、action有4種(控制各方向引擎)。  \n",
    "相較於Bi-pedalwalker，Lunar-lander的observation較少，且4種action皆為離散值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此遊戲內，我們採取與前述cartpole截然不同的演算法，以Deep Q learning概念為太空船實現最佳降落策略。  \n",
    "Deep Q learning的核心概念在訓練模型之時，除了要求模型關注現階段狀態，也要求關注未來的狀態。  \n",
    "而為了解決前面所提到Q-funtion不含有reward的問題，我們將神經網絡改為輸入state與action來預測reward。也就是在當前的state之下，配上四種不同的action來預測reward，再選擇預測出最大reward的action。而為了同時以state與action作為輸入，因此也用到了Functional API的概念。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
