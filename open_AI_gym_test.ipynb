{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    if np.random.random_sample() < epsilon: # 有 ε 的機率會選擇隨機 action\n",
    "        return action_space.sample() \n",
    "    else: # 其他時間根據現有 policy 選擇 action，也就是在 Q table 裡目前 state 中，選擇擁有最大 Q value 的 action\n",
    "        return np.argmax(q_table[state]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation, n_buckets, state_bounds):\n",
    "    state = [0] * len(observation) \n",
    "    for i, s in enumerate(observation): # 每個 feature 有不同的分配\n",
    "        l, u = state_bounds[i][0], state_bounds[i][1] # 每個 feature 值的範圍上下限\n",
    "        if s <= l: # 低於下限，分配為 0\n",
    "            state[i] = 0\n",
    "        elif s >= u: # 高於上限，分配為最大值\n",
    "            state[i] = n_buckets[i] - 1\n",
    "        else: # 範圍內，依比例分配\n",
    "            state[i] = int(((s - l) / (u - l)) * n_buckets[i])\n",
    "\n",
    "    return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 41 timesteps, total rewards 41.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 21 timesteps, total rewards 21.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 33 timesteps, total rewards 33.0\n",
      "Episode finished after 66 timesteps, total rewards 66.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 47 timesteps, total rewards 47.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 49 timesteps, total rewards 49.0\n",
      "Episode finished after 27 timesteps, total rewards 27.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 23 timesteps, total rewards 23.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 32 timesteps, total rewards 32.0\n",
      "Episode finished after 33 timesteps, total rewards 33.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 36 timesteps, total rewards 36.0\n",
      "Episode finished after 32 timesteps, total rewards 32.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 37 timesteps, total rewards 37.0\n",
      "Episode finished after 61 timesteps, total rewards 61.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 50 timesteps, total rewards 50.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 31 timesteps, total rewards 31.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 28 timesteps, total rewards 28.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 37 timesteps, total rewards 37.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 40 timesteps, total rewards 40.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 8 timesteps, total rewards 8.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 81 timesteps, total rewards 81.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 38 timesteps, total rewards 38.0\n",
      "Episode finished after 23 timesteps, total rewards 23.0\n",
      "Episode finished after 57 timesteps, total rewards 57.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 90 timesteps, total rewards 90.0\n",
      "Episode finished after 35 timesteps, total rewards 35.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 80 timesteps, total rewards 80.0\n",
      "Episode finished after 29 timesteps, total rewards 29.0\n",
      "Episode finished after 45 timesteps, total rewards 45.0\n",
      "Episode finished after 123 timesteps, total rewards 123.0\n",
      "Episode finished after 25 timesteps, total rewards 25.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 31 timesteps, total rewards 31.0\n",
      "Episode finished after 47 timesteps, total rewards 47.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 76 timesteps, total rewards 76.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 68 timesteps, total rewards 68.0\n",
      "Episode finished after 112 timesteps, total rewards 112.0\n",
      "Episode finished after 30 timesteps, total rewards 30.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 30 timesteps, total rewards 30.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 85 timesteps, total rewards 85.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 28 timesteps, total rewards 28.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 101 timesteps, total rewards 101.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 114 timesteps, total rewards 114.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 161 timesteps, total rewards 161.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 69 timesteps, total rewards 69.0\n",
      "Episode finished after 30 timesteps, total rewards 30.0\n",
      "Episode finished after 42 timesteps, total rewards 42.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 48 timesteps, total rewards 48.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 95 timesteps, total rewards 95.0\n",
      "Episode finished after 8 timesteps, total rewards 8.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 150 timesteps, total rewards 150.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 71 timesteps, total rewards 71.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 131 timesteps, total rewards 131.0\n",
      "Episode finished after 25 timesteps, total rewards 25.0\n",
      "Episode finished after 101 timesteps, total rewards 101.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 85 timesteps, total rewards 85.0\n",
      "Episode finished after 8 timesteps, total rewards 8.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 151 timesteps, total rewards 151.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 31 timesteps, total rewards 31.0\n",
      "Episode finished after 32 timesteps, total rewards 32.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 準備 Q table\n",
    "## Environment 中各個 feature 的 bucket 分配數量\n",
    "## 1 代表任何值皆表同一 state，也就是這個 feature 其實不重要\n",
    "n_buckets = (1, 1, 6, 3)\n",
    "\n",
    "## Action 數量 \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "## State 範圍 \n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "## Q table，每個 state-action pair 存一值 \n",
    "q_table = np.zeros(n_buckets + (n_actions,))\n",
    "\n",
    "# 一些學習過程中的參數\n",
    "get_epsilon = lambda i: max(0.01, min(1, 1.0 - math.log10((i+1)/25)))  # epsilon-greedy; 隨時間遞減\n",
    "get_lr = lambda i: max(0.01, min(0.5, 1.0 - math.log10((i+1)/25))) # learning rate; 隨時間遞減 \n",
    "gamma = 0.99 # reward discount factor\n",
    "\n",
    "# Q-learning\n",
    "for i_episode in range(200):\n",
    "    epsilon = get_epsilon(i_episode)\n",
    "    lr = get_lr(i_episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    rewards = 0\n",
    "    state = get_state(observation, n_buckets, state_bounds) # 將連續值轉成離散 \n",
    "    for t in range(250):\n",
    "        env.render()\n",
    "\n",
    "        action = choose_action(state, q_table, env.action_space, epsilon)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards += reward\n",
    "        next_state = get_state(observation, n_buckets, state_bounds)\n",
    "\n",
    "        # 更新 Q table\n",
    "        q_next_max = np.amax(q_table[next_state]) # 進入下一個 state 後，預期得到最大總 reward\n",
    "        q_table[state + (action,)] += lr * (reward + gamma * q_next_max - q_table[state + (action,)]) # 就是那個公式\n",
    "\n",
    "        # 前進下一 state \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
