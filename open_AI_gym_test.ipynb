{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    if np.random.random_sample() < epsilon: # 有 ε 的機率會選擇隨機 action\n",
    "        return action_space.sample() \n",
    "    else: # 其他時間根據現有 policy 選擇 action，也就是在 Q table 裡目前 state 中，選擇擁有最大 Q value 的 action\n",
    "        return np.argmax(q_table[state]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation, n_buckets, state_bounds):\n",
    "    state = [0] * len(observation) \n",
    "    for i, s in enumerate(observation): # 每個 feature 有不同的分配\n",
    "        l, u = state_bounds[i][0], state_bounds[i][1] # 每個 feature 值的範圍上下限\n",
    "        if s <= l: # 低於下限，分配為 0\n",
    "            state[i] = 0\n",
    "        elif s >= u: # 高於上限，分配為最大值\n",
    "            state[i] = n_buckets[i] - 1\n",
    "        else: # 範圍內，依比例分配\n",
    "            state[i] = int(((s - l) / (u - l)) * n_buckets[i])\n",
    "\n",
    "    return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 55 timesteps, total rewards 55.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 42 timesteps, total rewards 42.0\n",
      "Episode finished after 31 timesteps, total rewards 31.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 34 timesteps, total rewards 34.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 42 timesteps, total rewards 42.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 76 timesteps, total rewards 76.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 27 timesteps, total rewards 27.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 25 timesteps, total rewards 25.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 35 timesteps, total rewards 35.0\n",
      "Episode finished after 56 timesteps, total rewards 56.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 67 timesteps, total rewards 67.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 32 timesteps, total rewards 32.0\n",
      "Episode finished after 40 timesteps, total rewards 40.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 64 timesteps, total rewards 64.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 41 timesteps, total rewards 41.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 33 timesteps, total rewards 33.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 42 timesteps, total rewards 42.0\n",
      "Episode finished after 40 timesteps, total rewards 40.0\n",
      "Episode finished after 25 timesteps, total rewards 25.0\n",
      "Episode finished after 39 timesteps, total rewards 39.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 56 timesteps, total rewards 56.0\n",
      "Episode finished after 27 timesteps, total rewards 27.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 19 timesteps, total rewards 19.0\n",
      "Episode finished after 26 timesteps, total rewards 26.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 114 timesteps, total rewards 114.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 9 timesteps, total rewards 9.0\n",
      "Episode finished after 24 timesteps, total rewards 24.0\n",
      "Episode finished after 73 timesteps, total rewards 73.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 84 timesteps, total rewards 84.0\n",
      "Episode finished after 25 timesteps, total rewards 25.0\n",
      "Episode finished after 17 timesteps, total rewards 17.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 96 timesteps, total rewards 96.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 49 timesteps, total rewards 49.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 53 timesteps, total rewards 53.0\n",
      "Episode finished after 51 timesteps, total rewards 51.0\n",
      "Episode finished after 29 timesteps, total rewards 29.0\n",
      "Episode finished after 23 timesteps, total rewards 23.0\n",
      "Episode finished after 51 timesteps, total rewards 51.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 88 timesteps, total rewards 88.0\n",
      "Episode finished after 18 timesteps, total rewards 18.0\n",
      "Episode finished after 100 timesteps, total rewards 100.0\n",
      "Episode finished after 22 timesteps, total rewards 22.0\n",
      "Episode finished after 109 timesteps, total rewards 109.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 56 timesteps, total rewards 56.0\n",
      "Episode finished after 28 timesteps, total rewards 28.0\n",
      "Episode finished after 20 timesteps, total rewards 20.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 123 timesteps, total rewards 123.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 45 timesteps, total rewards 45.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 34 timesteps, total rewards 34.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 101 timesteps, total rewards 101.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 33 timesteps, total rewards 33.0\n",
      "Episode finished after 55 timesteps, total rewards 55.0\n",
      "Episode finished after 11 timesteps, total rewards 11.0\n",
      "Episode finished after 13 timesteps, total rewards 13.0\n",
      "Episode finished after 46 timesteps, total rewards 46.0\n",
      "Episode finished after 12 timesteps, total rewards 12.0\n",
      "Episode finished after 58 timesteps, total rewards 58.0\n",
      "Episode finished after 15 timesteps, total rewards 15.0\n",
      "Episode finished after 73 timesteps, total rewards 73.0\n",
      "Episode finished after 21 timesteps, total rewards 21.0\n",
      "Episode finished after 31 timesteps, total rewards 31.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 198 timesteps, total rewards 198.0\n",
      "Episode finished after 14 timesteps, total rewards 14.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 111 timesteps, total rewards 111.0\n",
      "Episode finished after 16 timesteps, total rewards 16.0\n",
      "Episode finished after 33 timesteps, total rewards 33.0\n",
      "Episode finished after 10 timesteps, total rewards 10.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n",
      "Episode finished after 200 timesteps, total rewards 200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 準備 Q table\n",
    "## Environment 中各個 feature 的 bucket 分配數量\n",
    "## 1 代表任何值皆表同一 state，也就是這個 feature 其實不重要\n",
    "n_buckets = (1, 1, 6, 3)\n",
    "\n",
    "## Action 數量 \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "## State 範圍 \n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "## Q table，每個 state-action pair 存一值 \n",
    "q_table = np.zeros(n_buckets + (n_actions,))\n",
    "\n",
    "# 一些學習過程中的參數\n",
    "get_epsilon = lambda i: max(0.01, min(1, 1.0 - math.log10((i+1)/25)))  # epsilon-greedy; 隨時間遞減\n",
    "get_lr = lambda i: max(0.01, min(0.5, 1.0 - math.log10((i+1)/25))) # learning rate; 隨時間遞減 \n",
    "gamma = 0.99 # reward discount factor\n",
    "\n",
    "# Q-learning\n",
    "for i_episode in range(200):\n",
    "    epsilon = get_epsilon(i_episode)\n",
    "    lr = get_lr(i_episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    rewards = 0\n",
    "    state = get_state(observation, n_buckets, state_bounds) # 將連續值轉成離散 \n",
    "    for t in range(250):\n",
    "        env.render()\n",
    "\n",
    "        action = choose_action(state, q_table, env.action_space, epsilon)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards += reward\n",
    "        next_state = get_state(observation, n_buckets, state_bounds)\n",
    "\n",
    "        # 更新 Q table\n",
    "        q_next_max = np.amax(q_table[next_state]) # 進入下一個 state 後，預期得到最大總 reward\n",
    "        q_table[state + (action,)] += lr * (reward + gamma * q_next_max - q_table[state + (action,)]) # 就是那個公式\n",
    "\n",
    "        # 前進下一 state \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
